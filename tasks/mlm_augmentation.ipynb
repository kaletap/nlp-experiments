{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dqp4A6lcASV6"
   },
   "source": [
    "# MLM augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sWtJcEfMpygI",
    "outputId": "afc295fb-457d-4d01-a7b1-c2110dc69080"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import os\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "vEFFI0Kdp4k6",
    "outputId": "1c11dfb8-0595-41f8-ab42-a6f88e0c556e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset yelp_polarity (/home/przemyslaw/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c)\n",
      "Loading cached split indices for dataset at /home/przemyslaw/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-5176ca1733b58ed4.arrow and /home/przemyslaw/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-884e3083fd8dff3c.arrow\n",
      "Loading cached split indices for dataset at /home/przemyslaw/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-442fdab3c84371d0.arrow and /home/przemyslaw/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-7584a169bbe4851a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5000, Validation size: 1000, Test size: 1000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-base', return_dict=True).eval()\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "# Taking only subset of data (faster training, fine-tuning the whole dataset takes ~20 hours per epoch)\n",
    "TRAIN_SIZE = 5_000\n",
    "VALID_SIZE = 1_000\n",
    "TEST_SIZE = 1_000\n",
    "\n",
    "dataset = load_dataset(\"yelp_polarity\", split=\"train\")\n",
    "train_test_split = dataset.train_test_split(train_size=TRAIN_SIZE, seed=RANDOM_SEED)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_val_dataset = train_test_split[\"test\"].train_test_split(train_size=VALID_SIZE, test_size=TEST_SIZE, seed=RANDOM_SEED)\n",
    "val_dataset, test_dataset = test_val_dataset[\"train\"], test_val_dataset[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I went to Fat Burger for the first  time last month. I ordered a medium fat burger with mushrooms, no mustard. The bun was crispy and warm and the burger itself was all around pretty good.'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train_dataset[27]['text']\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level (substitution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<mask>', 50264)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_id = 50_264\n",
    "tokenizer.mask_token, mask_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to Fat Burger for the first  time last month --> I <mask> to Fat Burger for <mask> first time last month\n",
      " I ordered a medium fat burger with mushrooms, no mustard -->  I <mask> a medium fat burger with mushrooms, no <mask> \n",
      " The bun was crispy and warm and the burger itself was all around pretty good -->  The bun <mask> crispy and warm and the burger itself was all around pretty good\n",
      " --> \n"
     ]
    }
   ],
   "source": [
    "def mask_word(word: str):\n",
    "    if word.endswith('.'):\n",
    "        return tokenizer.mask_token + ' .'\n",
    "    return tokenizer.mask_token\n",
    "\n",
    "words = np.array(text.split())\n",
    "p = 0.15\n",
    "n_mask = int(len(words) * p)\n",
    "masked_indices = np.sort(np.random.choice(len(words), size=n_mask))\n",
    "\n",
    "# words[masked_indices] = tokenizer.mask_token\n",
    "words = np.array([mask_word(word) if i in masked_indices else word for i, word in enumerate(words)])\n",
    "masked_text = \" \".join(words)\n",
    "\n",
    "sentences1 = text.split('.')\n",
    "sentences2 = masked_text.split('.')\n",
    "\n",
    "for s1, s2 in zip(sentences1, sentences2):\n",
    "    print(s1, '-->', s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 42, 50265])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_output = tokenizer([masked_text])\n",
    "input_ids, attention_mask = torch.tensor(tokenizer_output['input_ids']), torch.tensor(tokenizer_output['attention_mask'])\n",
    "output = model(input_ids)\n",
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   100, 50264,     7, 11289, 17971,    13, 50264,    78,    86,\n",
       "            94,   353,     4,    38, 50264,    10,  4761,  5886, 18079,    19,\n",
       "         25038,     6,   117, 50264,   479,    20, 15713, 50264, 32042,     8,\n",
       "          3279,     8,     5, 18079,  1495,    21,    70,   198,  1256,   205,\n",
       "             4,     2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50265])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' went', ' the', ' had', ' onions', ' was']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_logits = output.logits[input_ids == mask_id]\n",
    "print(predicted_logits.shape)\n",
    "predicted_tokens = predicted_logits.argmax(1)\n",
    "predicted_words = [tokenizer.decode(token.item()) for token in predicted_tokens]\n",
    "predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = words\n",
    "new_words[masked_indices] = predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to Fat Burger for the first  time last month\n",
      "I <mask> to Fat Burger for <mask> first time last month\n",
      "I  went to Fat Burger for  the first time last month\n",
      "\n",
      " I ordered a medium fat burger with mushrooms, no mustard\n",
      " I <mask> a medium fat burger with mushrooms, no <mask> \n",
      " I  had a medium fat burger with mushrooms, no  onions The bun  was crispy and warm and the burger itself was all around pretty good\n",
      "\n",
      " The bun was crispy and warm and the burger itself was all around pretty good\n",
      " The bun <mask> crispy and warm and the burger itself was all around pretty good\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_text = ' '.join(new_words)\n",
    "\n",
    "sentences1 = text.split('.')\n",
    "sentences2 = masked_text.split('.')\n",
    "sentences3 = augmented_text.split('.')\n",
    "\n",
    "for s1, s2, s3 in zip(sentences1, sentences2, sentences3):\n",
    "    print(s1)\n",
    "    print(s2)\n",
    "    print(s3)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([77,  1,  3,  2, 77])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 3, 2])\n",
    "np.insert(x, [0, 3], 77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bun was crispy and warm and the burger itself was all around pretty good --> The bun was <mask> crispy and warm and the burger <mask> itself was all around pretty good\n"
     ]
    }
   ],
   "source": [
    "text = \"The bun was crispy and warm and the burger itself was all around pretty good\"\n",
    "words = np.array(text.split())\n",
    "p = 0.15\n",
    "n_mask = int(len(words) * p)\n",
    "masked_indices = np.sort(np.random.choice(len(words) + 1, size=n_mask))\n",
    "\n",
    "# words[masked_indices] = tokenizer.mask_token\n",
    "masked_words = np.insert(words, masked_indices, tokenizer.mask_token)\n",
    "masked_text = \" \".join(masked_words)\n",
    "\n",
    "sentences1 = text.split('.')\n",
    "sentences2 = masked_text.split('.')\n",
    "\n",
    "for s1, s2 in zip(sentences1, sentences2):\n",
    "    print(s1, '-->', s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The bun was <mask> crispy and warm and the burger <mask> itself was all around pretty good'"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' very', ' in']"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_output = tokenizer([masked_text])\n",
    "input_ids, attention_mask = torch.tensor(tokenizer_output['input_ids']), torch.tensor(tokenizer_output['attention_mask'])\n",
    "output = model(input_ids)\n",
    "\n",
    "predicted_logits = output.logits[input_ids == mask_id]\n",
    "predicted_tokens = predicted_logits.argmax(1)\n",
    "predicted_words = [tokenizer.decode(token.item()) for token in predicted_tokens]\n",
    "predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The bun was  very crispy and warm and the burger  in itself was all around pretty good'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_words = np.insert(words, masked_indices, predicted_words)\n",
    "new_text = \" \".join(new_words)\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' very', ' in']"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bun was crispy and warm and the burger itself was all around pretty good\n",
      "The bun was <mask> crispy and warm and the burger <mask> itself was all around pretty good\n",
      "The bun was  very crispy and warm and the burger  in itself was all around pretty good\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences1 = text.split('.')\n",
    "sentences2 = masked_text.split('.')\n",
    "sentences3 = new_text.split('.')\n",
    "\n",
    "for s1, s2, s3 in zip(sentences1, sentences2, sentences3):\n",
    "    print(s1)\n",
    "    print(s2)\n",
    "    print(s3)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most probable words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' very', ' in']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_probas = predicted_logits.softmax(1)\n",
    "predicted_probas.shape\n",
    "predicted_probas.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ġvery',\n",
       " 'Ġsuper',\n",
       " 'Ġreally',\n",
       " 'Ġpretty',\n",
       " 'Ġquite',\n",
       " 'Ġboth',\n",
       " 'Ġnicely',\n",
       " 'Ġperfectly',\n",
       " 'Ġsurprisingly',\n",
       " 'Ġall',\n",
       " 'Ġincredibly',\n",
       " 'Ġstill',\n",
       " 'Ġfairly',\n",
       " 'Ġwonderfully',\n",
       " 'Ġextremely',\n",
       " 'Ġnice',\n",
       " 'Ġplenty',\n",
       " 'Ġpleasantly',\n",
       " 'Ġdefinitely',\n",
       " 'Ġparticularly')"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk = 20\n",
    "vocab_words = list(tokenizer.get_vocab().keys())\n",
    "most_probable = heapq.nlargest(topk, zip(vocab_words, predicted_probas[0].tolist()),  key=lambda t: t[1])\n",
    "words, probas = zip(*most_probable)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining augmentation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMInsertionAugmenter:\n",
    "    def __init__(self, model, tokenizer, p: float, min_mask: int = 1, topk: int = 5, uniform: bool = False, device=None):\n",
    "        self.model = model.eval()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_words = list(tokenizer.get_vocab().keys())\n",
    "        self.mask_token = tokenizer.mask_token\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "        self.topk = topk\n",
    "        self.min_mask = min_mask\n",
    "        self.uniform = uniform\n",
    "        self.p = p\n",
    "        self.device = device or torch.device('cpu')\n",
    "        \n",
    "    def __call__(self, text: str):\n",
    "        words = np.array(text.split(), dtype='object')\n",
    "        n_mask = max(self.min_mask, int(len(words) * self.p))\n",
    "        masked_indices = np.sort(np.random.choice(len(words) + 1, size=n_mask))\n",
    "\n",
    "        masked_words = np.insert(words, masked_indices, self.mask_token)\n",
    "        masked_text = \" \".join(masked_words)\n",
    "        \n",
    "        tokenizer_output = self.tokenizer([masked_text])\n",
    "        input_ids = torch.tensor(tokenizer_output['input_ids']).to(self.device)\n",
    "        attention_mask = torch.tensor(tokenizer_output['attention_mask']).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_ids)\n",
    "            predicted_logits = output.logits[input_ids == self.mask_token_id]\n",
    "            predicted_probas = predicted_logits.softmax(1)\n",
    "            \n",
    "        predicted_words = [self.sample_word(probas).strip() for probas in predicted_probas]\n",
    "        \n",
    "        new_words = np.insert(words, masked_indices, predicted_words)\n",
    "        new_text = \" \".join(new_words)\n",
    "        return new_text\n",
    "    \n",
    "    \n",
    "    def sample_word(self, predicted_probas):\n",
    "        if hasattr(predicted_probas, 'tolist'):\n",
    "            predicted_probas = predicted_probas.tolist()\n",
    "        most_probable = heapq.nlargest(self.topk, zip(self.vocab_words, predicted_probas),  key=lambda t: t[1])\n",
    "        words, probas = zip(*most_probable)\n",
    "        word = random.choice(words) if self.uniform else random.choices(words, weights=probas)[0]\n",
    "        return self.tokenizer.convert_tokens_to_string(word).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warning: weird behaviour of np.insert\n",
    "np.insert cuts off words\n",
    "\n",
    "Why?\n",
    "\n",
    "See type of words ('<U4')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I', '<mas', 'you', 'very', '<mas', 'much'], dtype='<U4')"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.array([\"I\", \"you\", \"very\", \"much\"])\n",
    "np.insert(words, np.array([1, 3]), \"<mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I', '<mask>', 'you', 'very', '<mask>', 'much'], dtype='>U6')"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.array([\"I\", \"you\", \"very\", \"much\"], dtype='>U6')\n",
    "np.insert(words, np.array([1, 3]), \"<mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = MLMInsertionAugmenter(model, tokenizer, p=0.2, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love seeing you.'"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmenter(\"I love you.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "colab_roberta_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
