{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-Encoder\n",
    "Trained using the code in https://github.com/kaletap/Sentence-VAE (fork of timbmg/Sentence-VAE github repo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../Sentence-VAE')\n",
    "from model import SentenceVAE\n",
    "from ptb import PTB\n",
    "from utils import idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params =  {\n",
    "     \"vocab_size\": 9877,\n",
    "     \"sos_idx\": 2,\n",
    "     \"eos_idx\": 3,\n",
    "     \"pad_idx\": 0,\n",
    "     \"unk_idx\": 1,\n",
    "     \"max_sequence_length\": 60,\n",
    "     \"embedding_size\": 300,\n",
    "     \"rnn_type\": \"gru\",\n",
    "     \"hidden_size\": 256,\n",
    "     \"word_dropout\": 0,\n",
    "     \"embedding_dropout\": 0.5,\n",
    "     \"latent_size\": 16,\n",
    "     \"num_layers\": 1,\n",
    "     \"bidirectional\": False\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceVAE(**params)\n",
    "model.load_state_dict(torch.load('../Sentence-VAE/bin/2020-Oct-08-13:44:02/E9.pytorch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceVAE(\n",
       "  (embedding): Embedding(9877, 300)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (decoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (outputs2vocab): Linear(in_features=256, out_features=9877, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Sentence-VAE/data/ptb.vocab.json', 'r') as file:\n",
    "    vocab = json.load(file)\n",
    "\n",
    "w2i, i2w = vocab['w2i'], vocab['i2w']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, z = model.inference(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 60])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the house version of the house ' s <unk> bill is a <unk> reminder <eos>\n",
      "the <unk> of the <unk> <unk> <unk> that has been <unk> by the <unk> of the <unk> and <unk> <unk> <unk> <unk> & <unk> <unk> <unk> & woods <eos>\n",
      "in a separate <unk> <unk> <unk> and <unk> <unk> & son of the <unk> group of the company ' s <unk> trucking corp . said it agreed to buy its <unk> businesses for a <unk> of <unk> <unk> <eos>\n",
      "the company said the company has been completed in the past year <eos>\n",
      "the company said it will be a <unk> of the <unk> of the <unk> of the <unk> of the <unk> <eos>\n",
      "the board is subject to approval of a definitive agreement between the company ' s common stock and warrants to sell the stock of the <unk> of the <unk> <eos>\n",
      "cross & trecker is a <unk> <unk> in newark n . y . in los angeles <eos>\n",
      "mr . <unk> says he believes the market is n't going to be done <eos>\n",
      "in a separate speech prepared by mr . bush ' s office of justice department ' s office in washington d . c . a . <unk> by moscow of the u . s . and foreign creditors <eos>\n",
      "i think i think i think i think it ' s a <unk> <unk> <eos>\n"
     ]
    }
   ],
   "source": [
    "print(*idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>']), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a classic problem in generated sentences: a lot of `<unk>` predictions. I think we should either disable `<unk>` prediction or use something like BPE tokenizer like in GPT or Roberta (or WordPiece tokenizer used in Bert). Besides, generated examples kind of suck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding-decoding\n",
    "We want to take any sentence, and obtain it's hidden representation ($z$, encoding). Then, we shall decode it and see how generated sentence looks like. That will allow us to work on hidden space and interpolate between two actual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, w2i: dict, max_sequence_length: int = 512):\n",
    "        self.w2i = w2i\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.tokenizer = TweetTokenizer(preserve_case=False)\n",
    "        \n",
    "    def tokenize(self, line: str):\n",
    "        words = self.tokenizer.tokenize(line)\n",
    "\n",
    "        model_input = ['<sos>'] + words\n",
    "        model_input = model_input[:self.max_sequence_length]\n",
    "\n",
    "        length = len(model_input)\n",
    "\n",
    "        model_input.extend(['<pad>'] * (self.max_sequence_length-length))\n",
    "\n",
    "        model_input = [self.w2i.get(w, self.w2i['<unk>']) for w in model_input]\n",
    "        \n",
    "        return {\n",
    "            'input': model_input,\n",
    "            'length': length\n",
    "        }\n",
    "    \n",
    "    def __call__(self, lines: list, device=None):\n",
    "        device = device or torch.device('cuda')\n",
    "        tokenized_lines = []\n",
    "        lengths = []\n",
    "        for line in lines:\n",
    "            tokenizer_output = self.tokenize(line)\n",
    "            tokenized_lines.append(tokenizer_output['input'])\n",
    "            lengths.append(tokenizer_output['length'])\n",
    "        return {\n",
    "            'input': torch.tensor(tokenized_lines).to(device),\n",
    "            'length': torch.tensor(lengths).to(device)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(w2i, max_sequence_length=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"you either believe seymour can do it again or you do n't\"\n",
    "sentence2 = \"he was previously vice president\"\n",
    "\n",
    "tokenizer_output = tokenizer([sentence1, sentence2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3910,  0.8752, -0.9751, -1.6617, -1.0646, -0.0298,  1.0742,  0.1252,\n",
      "         -0.8899, -0.8038, -0.7011, -0.8372, -0.2074,  0.1503,  0.2320, -0.3837],\n",
      "        [ 0.0779,  0.8346, -0.7624,  0.6026, -0.1528, -0.2572,  0.1297,  1.2691,\n",
      "         -0.6488,  0.1550,  0.8086,  0.0057, -0.1798, -0.0842,  0.2896,  0.1095]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "it ' s not going to be done <eos>\n",
      "he said the company was n't disclosed <eos>\n"
     ]
    }
   ],
   "source": [
    "logp, mean, logv, z = model(tokenizer_output['input'], tokenizer_output['length'])\n",
    "samples, z = model.inference(z=mean)\n",
    "print(z)\n",
    "print(*idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>']), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviations of these samples is rather large, but less than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7512, 0.8605, 0.7109, 0.6675, 0.7622, 0.8093, 0.7694, 0.7702, 0.6683,\n",
       "         0.7675, 0.7924, 0.7573, 0.8994, 0.5838, 0.8782, 0.7484],\n",
       "        [0.7555, 0.8419, 0.7829, 0.6717, 0.8348, 0.8340, 0.7731, 0.8177, 0.7462,\n",
       "         0.8183, 0.8166, 0.7111, 0.8510, 0.6047, 0.8640, 0.8839]],\n",
       "       device='cuda:0', grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = torch.exp(0.5 * logv)\n",
    "std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
