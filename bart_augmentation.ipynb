{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bart model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, BartTokenizer, BartForConditionalGeneration, BartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large', return_dict=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mask>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'encoder_last_hidden_state'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"I've met <mask> is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've met him is cute\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas = outputs.logits.squeeze()\n",
    "tokens = probas.argmax(1)\n",
    "tokenizer.decode(tokens[1:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(masked_text: str, model, tokenizer):\n",
    "    inputs = tokenizer(masked_text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    print(outputs.logits.shape)\n",
    "    probas = outputs.logits.squeeze()\n",
    "    tokens = probas.argmax(1)\n",
    "    return tokenizer.decode(tokens[1:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset yelp_polarity (/home/przemyslaw/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp = load_dataset(\"yelp_polarity\", split=\"train\")\n",
    "text = yelp[0][\"text\"]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Unfortunately, <mask> Dr. Goldberg's patient is a <mask> of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 33, 50265])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, the experience. Goldberg's patient is a womanch the experience I've had with so many other doctors in NYC -- good doctor, terrible staff. I\""
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(text, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Better way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, the experience with Dr. Goldberg reminds me of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.\""
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(text, max_length=1024, return_tensors='pt')\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=2, max_length=100, early_stopping=True)[0]\n",
    "tokenizer.decode(summary_ids[2:], skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 16861,     6,     5, 50264,   925,     4, 18835,    18,  3186,\n",
       "            16,    10, 50264,     9,     5,   676,    38,   348,    56,    19,\n",
       "            98,   171,    97,  3333,    11, 14415,   480,   205,  3299,     6,\n",
       "          6587,   813,     4,     2]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>Unfortunately,<mask> Dr. Goldberg's patient is a<mask> of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.</s>\""
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 16861, 16861,     6,     5,   676,     9,   925,     4, 18835,\n",
       "            18,  3186,    16,    10,  5177, 16254,   119,     9,     5,   676,\n",
       "            38,   348,    56,    19,    98,   171,    97,  3333,    11, 14415,\n",
       "           480,   205,  3299,     6,  6587,   813,     4,     2]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noising "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A wrapper around TokenBlockDataset for BART dataset.\n",
      "Args:\n",
      "    dataset (TokenBlockDataset): dataset to wrap\n",
      "    sizes (List[int]): sentence lengths\n",
      "    vocab (~fairseq.data.Dictionary): vocabulary\n",
      "    mask_idx (int): dictionary index used for masked token\n",
      "    mask_whole_words: only mask whole words. This should be a byte mask\n",
      "        over vocab indices, indicating whether it is the beginning of a\n",
      "        word. We will extend any mask to encompass the whole word.\n",
      "    shuffle (bool, optional): shuffle the elements before batching.\n",
      "      Default: ``True``\n",
      "    seed: Seed for random number generator for reproducibility.\n",
      "    args: argparse arguments.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data import DenoisingDataset\n",
    "print(\n",
    "\"\"\"\n",
    "A wrapper around TokenBlockDataset for BART dataset.\n",
    "Args:\n",
    "    dataset (TokenBlockDataset): dataset to wrap\n",
    "    sizes (List[int]): sentence lengths\n",
    "    vocab (~fairseq.data.Dictionary): vocabulary\n",
    "    mask_idx (int): dictionary index used for masked token\n",
    "    mask_whole_words: only mask whole words. This should be a byte mask\n",
    "        over vocab indices, indicating whether it is the beginning of a\n",
    "        word. We will extend any mask to encompass the whole word.\n",
    "    shuffle (bool, optional): shuffle the elements before batching.\n",
    "      Default: ``True``\n",
    "    seed: Seed for random number generator for reproducibility.\n",
    "    args: argparse arguments.\n",
    "\"\"\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to noise the dataset?\n",
    "According to the original paper, text infilling works the best. That means, that we take random places in text, choose random (with Poisson(lambda=3) distribution) sequence length to mask and replace it with single `<mask>` token. The question is how to choose places to start, how many of them should there be etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.\""
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish how many places to choose for masking\n",
    "On average, we want fraction of words splitted. We assume Poisson(2) distributution for number of words for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 words, 5 to mask (0.2 fraction). 2 places to insert <mask>\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 2.5\n",
    "\n",
    "words = np.array(text.split(), dtype='object')\n",
    "fraction = 0.2\n",
    "n_mask = int(fraction*len(words))\n",
    "n_places = max(1, round(n_mask / lambda_))\n",
    "n_places\n",
    "print(f\"{len(words)} words, {n_mask} to mask ({fraction} fraction). {n_places} places to insert <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose words to mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.random.poisson(lambda_, size=n_places)\n",
    "\n",
    "places = np.sort(np.random.choice(10, size=n_places, replace=False))\n",
    "\n",
    "ends = {start: start + length for start, length in zip(places, lengths)}\n",
    "to_mask = {start + i for start, length in zip(places, lengths) for i in range(length)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, <mask> frustration of being Dr. Goldberg's <mask> a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.\""
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\" \".join(words))\n",
    "\n",
    "masked_words = list()\n",
    "i = 0\n",
    "while i < len(words):\n",
    "    if i in ends:\n",
    "        if len(masked_words) == 0 or masked_words[-1] != tokenizer.mask_token:\n",
    "            masked_words.append(tokenizer.mask_token)\n",
    "            i = ends[i]\n",
    "        else:\n",
    "            masked_words.append(words[i])\n",
    "            i += 1\n",
    "    elif i in to_mask:\n",
    "        i += 1\n",
    "    else:\n",
    "        masked_words.append(words[i])\n",
    "        i += 1\n",
    "\n",
    "masked_text = \" \".join(masked_words)\n",
    "masked_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, <mask> frustration of being Dr. Goldberg's <mask> a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.\""
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(masked_text, max_length=1024, return_tensors='pt')\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=2, max_length=512, early_stopping=True)[0]\n",
    "generated_text = tokenizer.decode(summary_ids[2:], skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because facebook researchers sometimes suffled sentences during pretraining, we see that Bart does it sometimes. Even though it wasn't relaly necessariy here. Oh well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.\""
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(masked_text, max_length=1024, return_tensors='pt')\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=1, max_length=512, early_stopping=True)[0]\n",
    "generated_text = tokenizer.decode(summary_ids[2:], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(generated_text == text)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, it seems that Bart have seen our examples during pretraining. Otherwise, how else would it be able to reconstruct the sentence so perfectly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartAugmenter:\n",
    "    def __init__(self, model=None, tokenizer=None, fraction: float = 0.2, min_mask: int = 1, max_mask: int = 100,\n",
    "                 lambda_: float = 2.5, num_beams: int = 1, device=None):\n",
    "        \"\"\"\n",
    "        :param model: huggingface/transformers model for masked language modeling\n",
    "            e.g model = BartForConditionalGeneration.from_pretrained('facebook/bart-large', return_dict=True)\n",
    "        :param tokenizer: huggingface/transformers tokenizer\n",
    "            e.g tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        :param fraction: fraction of words to insert\n",
    "        :param min_mask: minimum number of <mask> tokens to insert\n",
    "        :param max_mask: maximum number ot tokens to mask\n",
    "        :param topk: number of top words to sample from\n",
    "        :param uniform: whether to sample uniformly from topk words (defaults to False)\n",
    "        :param device: torch.device\n",
    "        \"\"\"\n",
    "        self.device = device or torch.device('cuda')\n",
    "        model = model or AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large', return_dict=True)\n",
    "        self.model = model.eval().to(self.device)\n",
    "        tokenizer = tokenizer or AutoTokenizer.from_pretrained('facebook/bart-large', use_fast=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_token = tokenizer.mask_token\n",
    "        self.min_mask = min_mask\n",
    "        self.max_mask = max_mask\n",
    "        self.fraction = fraction\n",
    "        self.lambda_ = lambda_\n",
    "        self.num_beams = num_beams\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        if self.fraction == 0:\n",
    "            return text\n",
    "        \n",
    "        words = text.split()\n",
    "        n_mask = max(self.min_mask, round(len(words) * self.fraction))\n",
    "        n_mask = min(n_mask, self.max_mask)\n",
    "        # offset, since lenght might increase after tokenization\n",
    "        max_masked_idx = min(self.tokenizer.model_max_length - 50, len(words))\n",
    "        n_places = max(1, round(n_mask / self.lambda_))\n",
    "        \n",
    "        places = np.sort(np.random.choice(len(words), size=n_places, replace=False))\n",
    "        lengths = np.random.poisson(self.lambda_, size=n_places)\n",
    "        ends = {start: start + length for start, length in zip(places, lengths)}\n",
    "        to_mask = {start + i for start, length in zip(places, lengths) for i in range(length)}\n",
    "        \n",
    "        masked_words = list()\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            if i in ends:\n",
    "                if len(masked_words) == 0 or masked_words[-1] != tokenizer.mask_token:\n",
    "                    masked_words.append(tokenizer.mask_token)\n",
    "                    i = ends[i]\n",
    "                else:\n",
    "                    masked_words.append(words[i])\n",
    "                    i += 1\n",
    "            elif i in to_mask:\n",
    "                i += 1\n",
    "            else:\n",
    "                masked_words.append(words[i])\n",
    "                i += 1\n",
    "\n",
    "        masked_text = \" \".join(masked_words)\n",
    "        inputs = tokenizer(masked_text, max_length=1024, return_tensors='pt')\n",
    "\n",
    "        # Generate seq2seq output\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(inputs['input_ids'], num_beams=self.num_beams, max_length=512, early_stopping=True)[0]\n",
    "            generated_text = tokenizer.decode(summary_ids[2:], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        \n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = BartAugmenter(model, tokenizer, device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love much'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmenter('I love you so much')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
