{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "pd.set_option('max_colwidth', 400)\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langauge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = '../deep-latent-sequence-model/pretrained_lm'\n",
    "yelp_path_0 = os.path.join(models_path, 'yelp_style0/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_LM(nn.Module):\n",
    "  \"\"\"LSTM language model\"\"\"\n",
    "  def __init__(self, model_init, emb_init, hparams):\n",
    "    super(LSTM_LM, self).__init__()\n",
    "    self.nh = hparams.d_model\n",
    "    # no padding when setting padding_idx to -1\n",
    "    self.embed = nn.Embedding(hparams.src_vocab_size, \n",
    "      hparams.d_word_vec, padding_idx=hparams.pad_id)\n",
    "\n",
    "    self.dropout_in = nn.Dropout(hparams.dropout_in)\n",
    "    self.dropout_out = nn.Dropout(hparams.dropout_out)\n",
    "\n",
    "    # concatenate z with input\n",
    "    self.lstm = nn.LSTM(input_size=hparams.d_word_vec,\n",
    "                 hidden_size=hparams.d_model,\n",
    "                 num_layers=1,\n",
    "                 batch_first=True)\n",
    "\n",
    "    # prediction layer\n",
    "    self.pred_linear = nn.Linear(self.nh, hparams.src_vocab_size, bias=True)\n",
    "\n",
    "    if hparams.tie_weight:\n",
    "        self.pred_linear.weight = self.embed.weight\n",
    "\n",
    "    self.loss = nn.CrossEntropyLoss(ignore_index=hparams.pad_id, reduction=\"none\")\n",
    "\n",
    "    self.reset_parameters(model_init, emb_init)\n",
    "\n",
    "  def reset_parameters(self, model_init, emb_init):\n",
    "    for param in self.parameters():\n",
    "      model_init(param)\n",
    "    emb_init(self.embed.weight)\n",
    "\n",
    "    self.pred_linear.bias.data.zero_()\n",
    "\n",
    "\n",
    "  def decode(self, x, x_len, gumbel_softmax=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x: (batch_size, seq_len)\n",
    "      x_len: list of x lengths\n",
    "    \"\"\"\n",
    "\n",
    "    # not predicting start symbol\n",
    "    # sents_len -= 1\n",
    "\n",
    "    if gumbel_softmax:\n",
    "      batch_size, seq_len, _ = x.size()\n",
    "      word_embed = x @ self.embed.weight\n",
    "    else:\n",
    "      batch_size, seq_len = x.size()\n",
    "\n",
    "      # (batch_size, seq_len, ni)\n",
    "      word_embed = self.embed(x)\n",
    "\n",
    "    word_embed = self.dropout_in(word_embed)\n",
    "    packed_embed = pack_padded_sequence(word_embed, x_len, batch_first=True)\n",
    "    \n",
    "    c_init = word_embed.new_zeros((1, batch_size, self.nh))\n",
    "    h_init = word_embed.new_zeros((1, batch_size, self.nh))\n",
    "    output, _ = self.lstm(packed_embed, (h_init, c_init))\n",
    "    output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "    output = self.dropout_out(output)\n",
    "\n",
    "    # (batch_size, seq_len, vocab_size)\n",
    "    output_logits = self.pred_linear(output)\n",
    "\n",
    "    return output_logits\n",
    "\n",
    "  def reconstruct_error(self, x, x_len, gumbel_softmax=False, x_mask=None):\n",
    "    \"\"\"Cross Entropy in the language case\n",
    "    Args:\n",
    "      x: (batch_size, seq_len)\n",
    "      x_len: list of x lengths\n",
    "      x_mask: required if gumbel_softmax is True, 1 denotes mask,\n",
    "              size (batch_size, seq_len)\n",
    "    Returns:\n",
    "      loss: (batch_size). Loss across different sentences\n",
    "    \"\"\"\n",
    "\n",
    "    #remove end symbol\n",
    "    src = x[:, :-1]\n",
    "\n",
    "    # remove start symbol\n",
    "    tgt = x[:, 1:]\n",
    "\n",
    "    if gumbel_softmax:\n",
    "      batch_size, seq_len, _ = src.size()\n",
    "    else:\n",
    "      batch_size, seq_len = src.size()\n",
    "\n",
    "    x_len = [s - 1 for s in x_len]\n",
    "\n",
    "    # (batch_size, seq_len, vocab_size)\n",
    "    output_logits = self.decode(src, x_len, gumbel_softmax)\n",
    "\n",
    "    if gumbel_softmax:\n",
    "      log_p = F.log_softmax(output_logits, dim=2)\n",
    "      x_mask = x_mask[:, 1:]\n",
    "      loss = -((log_p * tgt).sum(dim=2) * (1. - x_mask)).sum(dim=1)\n",
    "    else:\n",
    "      tgt = tgt.contiguous().view(-1)\n",
    "      # (batch_size * seq_len)\n",
    "      loss = self.loss(output_logits.view(-1, output_logits.size(2)),\n",
    "                 tgt)\n",
    "      loss = loss.view(batch_size, -1).sum(-1)\n",
    "\n",
    "\n",
    "    # (batch_size)\n",
    "    return loss\n",
    "\n",
    "  def compute_gumbel_logits(self, x, x_len):\n",
    "    \"\"\"Cross Entropy in the language case\n",
    "    Args:\n",
    "      x: (batch_size, seq_len)\n",
    "      x_len: list of x lengths\n",
    "      x_mask: required if gumbel_softmax is True, 1 denotes mask,\n",
    "              size (batch_size, seq_len)\n",
    "    Returns:\n",
    "      loss: (batch_size). Loss across different sentences\n",
    "    \"\"\"\n",
    "\n",
    "    #remove end symbol\n",
    "    src = x[:, :-1]\n",
    "\n",
    "    batch_size, seq_len, _ = src.size()\n",
    "\n",
    "    x_len = [s - 1 for s in x_len]\n",
    "\n",
    "    # (batch_size, seq_len, vocab_size)\n",
    "    output_logits = self.decode(src, x_len, True)\n",
    "\n",
    "    # (batch_size)\n",
    "    return output_logits\n",
    "\n",
    "  def log_probability(self, x, x_len, gumbel_softmax=False, x_mask=None):\n",
    "    \"\"\"Cross Entropy in the language case\n",
    "    Args:\n",
    "      x: (batch_size, seq_len)\n",
    "    Returns:\n",
    "      log_p: (batch_size).\n",
    "    \"\"\"\n",
    "\n",
    "    return -self.reconstruct_error(x, x_len, gumbel_softmax, x_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = torch.load(yelp_path_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_LM(\n",
       "  (embed): Embedding(9653, 128, padding_idx=0)\n",
       "  (dropout_in): Dropout(p=0.3, inplace=False)\n",
       "  (dropout_out): Dropout(p=0.3, inplace=False)\n",
       "  (lstm): LSTM(128, 512, batch_first=True)\n",
       "  (pred_linear): Linear(in_features=512, out_features=9653, bias=True)\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language model used above is a simple LSTM network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
