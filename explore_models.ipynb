{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "pd.set_option('max_colwidth', 400)\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langauge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = '../deep-latent-sequence-model/pretrained_lm'\n",
    "yelp_path_0 = os.path.join(models_path, 'yelp_style0/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_LM(nn.Module):\n",
    "  \"\"\"LSTM language model\"\"\"\n",
    "  def __init__(self, model_init, emb_init, hparams):\n",
    "    super(LSTM_LM, self).__init__()\n",
    "    self.nh = hparams.d_model\n",
    "    # no padding when setting padding_idx to -1\n",
    "    self.embed = nn.Embedding(hparams.src_vocab_size, \n",
    "      hparams.d_word_vec, padding_idx=hparams.pad_id)\n",
    "\n",
    "    self.dropout_in = nn.Dropout(hparams.dropout_in)\n",
    "    self.dropout_out = nn.Dropout(hparams.dropout_out)\n",
    "\n",
    "    # concatenate z with input\n",
    "    self.lstm = nn.LSTM(input_size=hparams.d_word_vec,\n",
    "                 hidden_size=hparams.d_model,\n",
    "                 num_layers=1,\n",
    "                 batch_first=True)\n",
    "\n",
    "    # prediction layer\n",
    "    self.pred_linear = nn.Linear(self.nh, hparams.src_vocab_size, bias=True)\n",
    "\n",
    "    if hparams.tie_weight:\n",
    "        self.pred_linear.weight = self.embed.weight\n",
    "\n",
    "    self.loss = nn.CrossEntropyLoss(ignore_index=hparams.pad_id, reduction=\"none\")\n",
    "\n",
    "    self.reset_parameters(model_init, emb_init)\n",
    "\n",
    "  def reset_parameters(self, model_init, emb_init):\n",
    "    for param in self.parameters():\n",
    "      model_init(param)\n",
    "    emb_init(self.embed.weight)\n",
    "\n",
    "    self.pred_linear.bias.data.zero_()\n",
    "\n",
    "\n",
    "  def decode(self, x, x_len, gumbel_softmax=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x: (batch_size, seq_len)\n",
    "      x_len: list of x lengths\n",
    "    \"\"\"\n",
    "\n",
    "    # not predicting start symbol\n",
    "    # sents_len -= 1\n",
    "\n",
    "    if gumbel_softmax:\n",
    "      batch_size, seq_len, _ = x.size()\n",
    "      word_embed = x @ self.embed.weight\n",
    "    else:\n",
    "      batch_size, seq_len = x.size()\n",
    "\n",
    "      # (batch_size, seq_len, ni)\n",
    "      word_embed = self.embed(x)\n",
    "\n",
    "    word_embed = self.dropout_in(word_embed)\n",
    "    packed_embed = pack_padded_sequence(word_embed, x_len, batch_first=True)\n",
    "    \n",
    "    c_init = word_embed.new_zeros((1, batch_size, self.nh))\n",
    "    h_init = word_embed.new_zeros((1, batch_size, self.nh))\n",
    "    output, _ = self.lstm(packed_embed, (h_init, c_init))\n",
    "    output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "    output = self.dropout_out(output)\n",
    "\n",
    "    # (batch_size, seq_len, vocab_size)\n",
    "    output_logits = self.pred_linear(output)\n",
    "\n",
    "    return output_logits\n",
    "\n",
    "  def reconstruct_error(self, x, x_len, gumbel_softmax=False, x_mask=None):\n",
    "    \"\"\"Cross Entropy in the language case\n",
    "    Args:\n",
    "      x: (batch_size, seq_len)\n",
    "      x_len: list of x lengths\n",
    "      x_mask: required if gumbel_softmax is True, 1 denotes mask,\n",
    "              size (batch_size, seq_len)\n",
    "    Returns:\n",
    "      loss: (batch_size). Loss across different sentences\n",
    "    \"\"\"\n",
    "\n",
    "    #remove end symbol\n",
    "    src = x[:, :-1]\n",
    "\n",
    "    # remove start symbol\n",
    "    tgt = x[:, 1:]\n",
    "\n",
    "    if gumbel_softmax:\n",
    "      batch_size, seq_len, _ = src.size()\n",
    "    else:\n",
    "      batch_size, seq_len = src.size()\n",
    "\n",
    "    x_len = [s - 1 for s in x_len]\n",
    "\n",
    "    # (batch_size, seq_len, vocab_size)\n",
    "    output_logits = self.decode(src, x_len, gumbel_softmax)\n",
    "\n",
    "    if gumbel_softmax:\n",
    "      log_p = F.log_softmax(output_logits, dim=2)\n",
    "      x_mask = x_mask[:, 1:]\n",
    "      loss = -((log_p * tgt).sum(dim=2) * (1. - x_mask)).sum(dim=1)\n",
    "    else:\n",
    "      tgt = tgt.contiguous().view(-1)\n",
    "      # (batch_size * seq_len)\n",
    "      loss = self.loss(output_logits.view(-1, output_logits.size(2)),\n",
    "                 tgt)\n",
    "      loss = loss.view(batch_size, -1).sum(-1)\n",
    "\n",
    "\n",
    "    # (batch_size)\n",
    "    return loss\n",
    "\n",
    "  def compute_gumbel_logits(self, x, x_len):\n",
    "    \"\"\"Cross Entropy in the language case\n",
    "    Args:\n",
    "      x: (batch_size, seq_len)\n",
    "      x_len: list of x lengths\n",
    "      x_mask: required if gumbel_softmax is True, 1 denotes mask,\n",
    "              size (batch_size, seq_len)\n",
    "    Returns:\n",
    "      loss: (batch_size). Loss across different sentences\n",
    "    \"\"\"\n",
    "\n",
    "    #remove end symbol\n",
    "    src = x[:, :-1]\n",
    "\n",
    "    batch_size, seq_len, _ = src.size()\n",
    "\n",
    "    x_len = [s - 1 for s in x_len]\n",
    "\n",
    "    # (batch_size, seq_len, vocab_size)\n",
    "    output_logits = self.decode(src, x_len, True)\n",
    "\n",
    "    # (batch_size)\n",
    "    return output_logits\n",
    "\n",
    "  def log_probability(self, x, x_len, gumbel_softmax=False, x_mask=None):\n",
    "    \"\"\"Cross Entropy in the language case\n",
    "    Args:\n",
    "      x: (batch_size, seq_len)\n",
    "    Returns:\n",
    "      log_p: (batch_size).\n",
    "    \"\"\"\n",
    "\n",
    "    return -self.reconstruct_error(x, x_len, gumbel_softmax, x_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = torch.load(yelp_path_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_LM(\n",
       "  (embed): Embedding(9653, 128, padding_idx=0)\n",
       "  (dropout_in): Dropout(p=0.3, inplace=False)\n",
       "  (dropout_out): Dropout(p=0.3, inplace=False)\n",
       "  (lstm): LSTM(128, 512, batch_first=True)\n",
       "  (pred_linear): Linear(in_features=512, out_features=9653, bias=True)\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language model used above is a simple LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassify(nn.Module):\n",
    "\n",
    "  def __init__(self, hparams):\n",
    "    super(CNNClassify, self).__init__()\n",
    "    self.hparams = hparams\n",
    "    self.word_emb = nn.Embedding(self.hparams.src_vocab_size,\n",
    "                                 self.hparams.d_word_vec,\n",
    "                                 padding_idx=hparams.pad_id)\n",
    "\n",
    "    self.conv_list = []\n",
    "    self.mask_conv_list = []\n",
    "    for c, k in zip(self.hparams.out_c_list, self.hparams.k_list):\n",
    "      #self.conv_list.append(nn.Conv1d(self.hparams.d_word_vec, out_channels=c, kernel_size=k, padding = k // 2))\n",
    "      self.conv_list.append(nn.Conv1d(self.hparams.d_word_vec, out_channels=c, kernel_size=k))\n",
    "      nn.init.uniform_(self.conv_list[-1].weight, -args.init_range, args.init_range)\n",
    "      self.mask_conv_list.append(nn.Conv1d(1, out_channels=c, kernel_size=k))\n",
    "      nn.init.constant_(self.mask_conv_list[-1].weight, 1.0)\n",
    "\n",
    "    self.conv_list = nn.ModuleList(self.conv_list)\n",
    "    self.mask_conv_list = nn.ModuleList(self.mask_conv_list)\n",
    "    for param in self.mask_conv_list.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "    self.project = nn.Linear(sum(self.hparams.out_c_list), self.hparams.trg_vocab_size, bias=False)\n",
    "    nn.init.uniform_(self.project.weight, -args.init_range, args.init_range)\n",
    "    if self.hparams.cuda:\n",
    "      self.conv_list = self.conv_list.cuda()\n",
    "      self.project = self.project.cuda()\n",
    "\n",
    "  def forward(self, x_train, x_mask, x_len, step=None):\n",
    "    batch_size, max_len = x_train.size()\n",
    "\n",
    "    # [batch_size, max_len, d_word_vec]\n",
    "    word_emb = self.word_emb(x_train)\n",
    "\n",
    "    #x_mask = x_mask.unsqueeze(1).float()\n",
    "    # [batch_size, d_word_vec, max_len]\n",
    "    word_emb = word_emb.permute(0, 2, 1)\n",
    "    conv_out = []\n",
    "    for conv, m_conv in zip(self.conv_list, self.mask_conv_list):\n",
    "      # [batch_size, c_out, max_len]\n",
    "      c = conv(word_emb)\n",
    "      #with torch.no_grad():\n",
    "      #  m = m_conv(x_mask)\n",
    "      #print(m_conv.weight)\n",
    "      #print(m)\n",
    "      #m = (m > 0)\n",
    "      #print(m)\n",
    "      #c.masked_fill_(m, -float(\"inf\"))\n",
    "      # [batch_size, c_out]\n",
    "      c = c.max(dim=-1)\n",
    "      conv_out.append(c[0])\n",
    "    # [batch_size, trg_vocab_size]\n",
    "    logits = self.project(torch.cat(conv_out, dim=-1))\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_path = '../deep-latent-sequence-model/pretrained_classifer/yelp/model.pt'\n",
    "sys.path.append('../deep-latent-sequence-model/src')\n",
    "sentiment_model = torch.load(classifier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNClassify(\n",
       "  (word_emb): Embedding(9653, 128, padding_idx=0)\n",
       "  (conv_list): ModuleList(\n",
       "    (0): Conv1d(128, 1, kernel_size=(3,), stride=(1,))\n",
       "    (1): Conv1d(128, 2, kernel_size=(3,), stride=(1,))\n",
       "    (2): Conv1d(128, 3, kernel_size=(3,), stride=(1,))\n",
       "    (3): Conv1d(128, 4, kernel_size=(3,), stride=(1,))\n",
       "  )\n",
       "  (mask_conv_list): ModuleList(\n",
       "    (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,))\n",
       "    (1): Conv1d(1, 2, kernel_size=(3,), stride=(1,))\n",
       "    (2): Conv1d(1, 3, kernel_size=(3,), stride=(1,))\n",
       "    (3): Conv1d(1, 4, kernel_size=(3,), stride=(1,))\n",
       "  )\n",
       "  (project): Linear(in_features=10, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification we use convolutional model: it is faster (I think) and effective. It achieves 97% accuracy on Yelp sentiment dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
