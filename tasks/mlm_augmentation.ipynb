{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dqp4A6lcASV6"
   },
   "source": [
    "# MLM augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sWtJcEfMpygI",
    "outputId": "afc295fb-457d-4d01-a7b1-c2110dc69080"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "vEFFI0Kdp4k6",
    "outputId": "1c11dfb8-0595-41f8-ab42-a6f88e0c556e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset yelp_polarity (/home/przemyslaw/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c)\n",
      "Loading cached split indices for dataset at /home/przemyslaw/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-5176ca1733b58ed4.arrow and /home/przemyslaw/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-884e3083fd8dff3c.arrow\n",
      "Loading cached split indices for dataset at /home/przemyslaw/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-442fdab3c84371d0.arrow and /home/przemyslaw/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/2b33212d89209ed1ea0522001bccc5f5a5c920dd9c326f3c828e67a22c51a98c/cache-7584a169bbe4851a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5000, Validation size: 1000, Test size: 1000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-base', return_dict=True).eval()\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "# Taking only subset of data (faster training, fine-tuning the whole dataset takes ~20 hours per epoch)\n",
    "TRAIN_SIZE = 5_000\n",
    "VALID_SIZE = 1_000\n",
    "TEST_SIZE = 1_000\n",
    "\n",
    "dataset = load_dataset(\"yelp_polarity\", split=\"train\")\n",
    "train_test_split = dataset.train_test_split(train_size=TRAIN_SIZE, seed=RANDOM_SEED)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_val_dataset = train_test_split[\"test\"].train_test_split(train_size=VALID_SIZE, test_size=TEST_SIZE, seed=RANDOM_SEED)\n",
    "val_dataset, test_dataset = test_val_dataset[\"train\"], test_val_dataset[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I went to Fat Burger for the first  time last month. I ordered a medium fat burger with mushrooms, no mustard. The bun was crispy and warm and the burger itself was all around pretty good.'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train_dataset[27]['text']\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level (substitution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<mask>', 50264)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_id = 50_264\n",
    "tokenizer.mask_token, mask_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to Fat Burger for the first  time last month --> <mask> went to <mask> Burger for the first time last month\n",
      " I ordered a medium fat burger with mushrooms, no mustard -->  I ordered a medium fat burger <mask> <mask> no mustard\n",
      " The bun was crispy and warm and the burger itself was all around pretty good -->  The bun was crispy and warm and the burger <mask> was all around pretty good\n",
      " --> \n"
     ]
    }
   ],
   "source": [
    "def mask_word(word: str):\n",
    "    if word.endswith('.'):\n",
    "        return tokenizer.mask_token + ' .'\n",
    "    return tokenizer.mask_token\n",
    "\n",
    "words = np.array(text.split())\n",
    "p = 0.15\n",
    "n_mask = int(len(words) * p)\n",
    "masked_indices = np.sort(np.random.choice(len(words), size=n_mask))\n",
    "\n",
    "# words[masked_indices] = tokenizer.mask_token\n",
    "words = np.array([mask_word(word) if i in masked_indices else word for i, word in enumerate(words)])\n",
    "masked_text = \" \".join(words)\n",
    "\n",
    "sentences1 = text.split('.')\n",
    "sentences2 = masked_text.split('.')\n",
    "\n",
    "for s1, s2 in zip(sentences1, sentences2):\n",
    "    print(s1, '-->', s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 41, 50265])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_output = tokenizer([masked_text])\n",
    "input_ids, attention_mask = torch.tensor(tokenizer_output['input_ids']), torch.tensor(tokenizer_output['attention_mask'])\n",
    "output = model(input_ids)\n",
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 50264,   439,     7, 50264, 17971,    13,     5,    78,    86,\n",
       "            94,   353,     4,    38,  2740,    10,  4761,  5886, 18079, 50264,\n",
       "         50264,   117, 27001,     4,    20, 15713,    21, 32042,     8,  3279,\n",
       "             8,     5, 18079, 50264,    21,    70,   198,  1256,   205,     4,\n",
       "             2]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50265])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I', ' American', ' that', ' had', ' itself']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_logits = output.logits[input_ids == mask_id]\n",
    "print(predicted_logits.shape)\n",
    "predicted_tokens = predicted_logits.argmax(1)\n",
    "predicted_words = [tokenizer.decode(token.item()) for token in predicted_tokens]\n",
    "predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = words\n",
    "new_words[masked_indices] = predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to Fat Burger for the first  time last month\n",
      "<mask> went to <mask> Burger for the first time last month\n",
      "I went to  America Burger for the first time last month\n",
      "\n",
      " I ordered a medium fat burger with mushrooms, no mustard\n",
      " I ordered a medium fat burger <mask> <mask> no mustard\n",
      " I ordered a medium fat burger  that  had no mustard\n",
      "\n",
      " The bun was crispy and warm and the burger itself was all around pretty good\n",
      " The bun was crispy and warm and the burger <mask> was all around pretty good\n",
      " The bun was crispy and warm and the burger  itself was all around pretty good\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_text = ' '.join(new_words)\n",
    "\n",
    "sentences1 = text.split('.')\n",
    "sentences2 = masked_text.split('.')\n",
    "sentences3 = augmented_text.split('.')\n",
    "\n",
    "for s1, s2, s3 in zip(sentences1, sentences2, sentences3):\n",
    "    print(s1)\n",
    "    print(s2)\n",
    "    print(s3)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([77,  1,  3,  2, 77])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 3, 2])\n",
    "np.insert(x, [0, 3], 77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to Fat Burger for the first  time last month --> I went <mask> to Fat Burger for the first time last month\n",
      " I ordered a medium fat burger with mushrooms, no mustard -->  I ordered a <mask> medium fat burger with mushrooms, no mustard\n",
      " The bun was crispy and warm and the burger itself was all around pretty good -->  The bun was crispy <mask> and warm <mask> and the burger itself was all around <mask> pretty good\n",
      " --> \n"
     ]
    }
   ],
   "source": [
    "words = np.array(text.split())\n",
    "p = 0.15\n",
    "n_mask = int(len(words) * p)\n",
    "masked_indices = np.sort(np.random.choice(len(words) + 1, size=n_mask))\n",
    "\n",
    "# words[masked_indices] = tokenizer.mask_token\n",
    "words = np.insert(words, masked_indices, tokenizer.mask_token)\n",
    "masked_text = \" \".join(words)\n",
    "\n",
    "sentences1 = text.split('.')\n",
    "sentences2 = masked_text.split('.')\n",
    "\n",
    "for s1, s2 in zip(sentences1, sentences2):\n",
    "    print(s1, '-->', s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' back', ' nice', ',', ',', ',']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_output = tokenizer([masked_text])\n",
    "input_ids, attention_mask = torch.tensor(tokenizer_output['input_ids']), torch.tensor(tokenizer_output['attention_mask'])\n",
    "output = model(input_ids)\n",
    "\n",
    "predicted_logits = output.logits[input_ids == mask_id]\n",
    "predicted_tokens = predicted_logits.argmax(1)\n",
    "predicted_words = [tokenizer.decode(token.item()) for token in predicted_tokens]\n",
    "predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = words\n",
    "new_words[masked_indices] = predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' back', ' nice', ',', ',', ',']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to Fat Burger for the first  time last month\n",
      "I went <mask> to Fat Burger for the first time last month\n",
      "I went  back to Fat Burger for the first time last month\n",
      "\n",
      " I ordered a medium fat burger with mushrooms, no mustard\n",
      " I ordered a <mask> medium fat burger with mushrooms, no mustard\n",
      " I ordered  nice <mask> medium fat burger with mushrooms, no mustard\n",
      "\n",
      " The bun was crispy and warm and the burger itself was all around pretty good\n",
      " The bun was crispy <mask> and warm <mask> and the burger itself was all around <mask> pretty good\n",
      " The bun , crispy , and warm <mask> and the burger , was all around <mask> pretty good\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_text = ' '.join(new_words)\n",
    "\n",
    "sentences1 = text.split('.')\n",
    "sentences2 = masked_text.split('.')\n",
    "sentences3 = augmented_text.split('.')\n",
    "\n",
    "for s1, s2, s3 in zip(sentences1, sentences2, sentences3):\n",
    "    print(s1)\n",
    "    print(s2)\n",
    "    print(s3)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "colab_roberta_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
