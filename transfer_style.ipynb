{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style transfer exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "sys.path.append('../deep-latent-sequence-model/src')\n",
    "from data_utils import DataUtil\n",
    "from utils import reorder\n",
    "os.chdir('/home/przemyslaw/text-style-transfer/deep-latent-sequence-model')  # sorry for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../deep-latent-sequence-model/outputs_yelp/yelp_wd0.0_wb0.0_ws0.0_an3_pool5_klw0.1_lr0.001_t0.01_lm_bt_hard_avglen'\n",
    "model_path = os.path.join(model_dir, 'model.pt')\n",
    "model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (word_emb): Embedding(9653, 128, padding_idx=0)\n",
       "    (layer): LSTM(128, 512, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "    (bridge): Linear(in_features=1024, out_features=512, bias=False)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): MlpAttn(\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (w_trg): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (w_att): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (ctx_to_readout): Linear(in_features=1536, out_features=512, bias=False)\n",
       "    (readout): Linear(in_features=512, out_features=9653, bias=False)\n",
       "    (word_emb): Embedding(9653, 128, padding_idx=0)\n",
       "    (attr_emb): Embedding(2, 128, padding_idx=0)\n",
       "    (layer): LSTMCell(1152, 512)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (enc_to_k): Linear(in_features=1024, out_features=512, bias=False)\n",
       "  (noise): NoiseLayer()\n",
       "  (LM0): LSTM_LM(\n",
       "    (embed): Embedding(9653, 128, padding_idx=0)\n",
       "    (dropout_in): Dropout(p=0.3, inplace=False)\n",
       "    (dropout_out): Dropout(p=0.3, inplace=False)\n",
       "    (lstm): LSTM(128, 512, batch_first=True)\n",
       "    (pred_linear): Linear(in_features=512, out_features=9653, bias=True)\n",
       "    (loss): CrossEntropyLoss()\n",
       "  )\n",
       "  (LM1): LSTM_LM(\n",
       "    (embed): Embedding(9653, 128, padding_idx=0)\n",
       "    (dropout_in): Dropout(p=0.3, inplace=False)\n",
       "    (dropout_out): Dropout(p=0.3, inplace=False)\n",
       "    (lstm): LSTM(128, 512, batch_first=True)\n",
       "    (pred_linear): Linear(in_features=512, out_features=9653, bias=True)\n",
       "    (loss): CrossEntropyLoss()\n",
       "  )\n",
       "  (LM): ModuleList(\n",
       "    (0): LSTM_LM(\n",
       "      (embed): Embedding(9653, 128, padding_idx=0)\n",
       "      (dropout_in): Dropout(p=0.3, inplace=False)\n",
       "      (dropout_out): Dropout(p=0.3, inplace=False)\n",
       "      (lstm): LSTM(128, 512, batch_first=True)\n",
       "      (pred_linear): Linear(in_features=512, out_features=9653, bias=True)\n",
       "      (loss): CrossEntropyLoss()\n",
       "    )\n",
       "    (1): LSTM_LM(\n",
       "      (embed): Embedding(9653, 128, padding_idx=0)\n",
       "      (dropout_in): Dropout(p=0.3, inplace=False)\n",
       "      (dropout_out): Dropout(p=0.3, inplace=False)\n",
       "      (lstm): LSTM(128, 512, batch_first=True)\n",
       "      (pred_linear): Linear(in_features=512, out_features=9653, bias=True)\n",
       "      (loss): CrossEntropyLoss()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams(object):\n",
    "    def __init__(self, **args):\n",
    "        self.pad = \"<pad>\"\n",
    "        self.unk = \"<unk>\"\n",
    "        self.bos = \"<s>\"\n",
    "        # self.eos = \"<\\s>\"\n",
    "        self.eos = \"</s>\"\n",
    "        self.pad_id = 0\n",
    "        self.unk_id = 1\n",
    "        self.bos_id = 2\n",
    "        self.eos_id = 3\n",
    "\n",
    "        self.batcher = \"sent\"\n",
    "        self.batch_size = 32\n",
    "        self.src_vocab_size = None\n",
    "        self.trg_vocab_size = None\n",
    "\n",
    "        self.inf = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationHparams(HParams):\n",
    "    dataset = \"Translate dataset\"\n",
    "    def __init__(self):\n",
    "        self.cuda = True\n",
    "        self.beam_size = 1\n",
    "        self.max_len = 300\n",
    "        self.batch_size = 32\n",
    "        self.merge_bpe = False\n",
    "        self.decode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = TranslationHparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file_name = os.path.join(model_dir, \"hparams.pt\")\n",
    "train_hparams = torch.load(hparams_file_name)\n",
    "hparams = TranslationHparams()\n",
    "for k, v in train_hparams.__dict__.items():\n",
    "    setattr(hparams, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid, x_mask, x_count, x_len, x_pos_emb_idxs, y_valid, y_mask, \\\n",
    "    y_count, y_len, y_pos_emb_idxs, y_neg, batch_size, end_of_epoch, index = data.next_test(test_batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/przemyslaw/.local/lib/python3.6/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "hs = model.translate(x_valid, x_mask, x_len, y_neg, y_mask, y_len, beam_size=hparams.beam_size, max_len=hparams.max_len, poly_norm_m=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = reorder(hs, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_best_words = map(lambda wi: data.trg_i2w_list[0][wi], hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great job all from me .',\n",
       " \"best service i 've ever experienced .\",\n",
       " 'always great and timely .',\n",
       " 'love this place , you have good quality and quality staff .',\n",
       " \"awesome , excellent service , they do n't have any good food .\"]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lists = map(lambda h: ' '.join([data.src_i2w[w] for w in h]), hs)\n",
    "list(word_lists)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        device='cuda:0'),\n",
       " tensor([   2, 1723,  174,    3,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0], device='cuda:0'))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pos_emb_idxs[-1], x_valid[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Let's be honest: DataUtil class sucks. It has a lot of useless stuff, all we need is a vocabulary and a way to encode words to token indices (and decode them back). That should be extremely easy, but it's not. Let's implement something like that.\n",
    "\n",
    "All we need is a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<unk>', '<s>', '</s>', 'i', 'was', 'sadly', 'mistaken', '.', 'so']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.src_i2w[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab: List[str]):\n",
    "        self.idx2word = vocab\n",
    "        self.word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "        \n",
    "    def convert_ids_to_tokens(self, token_ids: Union[int, List[int]]):\n",
    "        if type(token_ids) == list:\n",
    "            return [self.idx2word[id_] for id_ in token_ids]\n",
    "        elif type(token_ids) == int:\n",
    "            return self.idx2word[token_ids]\n",
    "        else:\n",
    "            raise TypeError(f'Type of ids should be either list or int but is {type(token_ids)}')\n",
    "            \n",
    "    def convert_tokens_to_ids(self, tokens: Union[str, List[str]]):\n",
    "        if type(tokens) == list:\n",
    "            return [self.word2idx.get(token, 1) for token in tokens]\n",
    "        elif type(tokens) == str:\n",
    "            return self.word2idx.get(tokens, 1)\n",
    "        else:\n",
    "            raise TypeError(f'Type of ids should be either list or str but is {type(tokens)}')\n",
    "            \n",
    "    def convert_tokens_to_string(self, tokens: List[str]):\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def decode(self, token_ids: List[int], skip_special_tokens: bool = False):\n",
    "        if skip_special_tokens:\n",
    "            token_ids = [id_ for id_ in token_ids if id_ >= 4]\n",
    "        token_ids = list(itertools.takewhile(lambda id_: id_ != 0, token_ids))  # getting rid of pad tokens\n",
    "        tokens = self.convert_ids_to_tokens(token_ids)\n",
    "        return self.convert_tokens_to_string(tokens)\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.word2idx\n",
    "    \n",
    "    def pad_single(self, token_ids: List[int], max_len: int):\n",
    "        n_pad = max_len - len(token_ids)\n",
    "        return token_ids + [0 for _ in range(n_pad)]\n",
    "    \n",
    "    def pad(self, token_ids: List[List[int]]):\n",
    "        max_len = max(len(ids) for ids in token_ids)\n",
    "        return [self.pad_single(ids, max_len) for ids in token_ids]\n",
    "    \n",
    "    def tokenize(self, text: str, padding=None):\n",
    "        \"\"\"Tokenizes a piece of text. Assumes that dots and commas etc. are taken care of before.\"\"\"\n",
    "        tokens = text.lower().split()\n",
    "        token_ids = self.convert_tokens_to_ids(tokens)\n",
    "        if padding and padding > 0:\n",
    "            n_pad = padding - len(token_ids)\n",
    "            token_ids = token_ids + [0 for _ in range(n_pad)]\n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(data.src_i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 103, 435, 154, 6179, 2444, 14, 41, 16, 170, 2337, 8, 0, 0]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.tokenize('I very much like research job , it is my passion .', padding=14)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i very much like research job , it is my passion .'"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming data into tensors\n",
    "We need some Dataset at this point. An observation: returning a tuple of many elements is bad: it's better to use dictionary instead. We do not necessarily want to use all elements of a tuple that authors of a code return and it's more difficult to change the API later on.\n",
    "\n",
    "```\n",
    "x_valid, x_mask, x_count, x_len, x_pos_emb_idxs, y_valid, y_mask, \\\n",
    "    y_count, y_len, y_pos_emb_idxs, y_neg, batch_size, end_of_epoch, index = data.next_test(test_batch_size=64)\n",
    "```\n",
    "\n",
    "The only things I will need later on is:\n",
    "```\n",
    "hs = model.translate(x_valid, x_mask, x_len, y_neg, y_mask, y_len, beam_size=hparams.beam_size, max_len=hparams.max_len, poly_norm_m=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(*args, device=None):\n",
    "    device = device or torch.device('cuda')\n",
    "    return [arg.to(device) for arg in args]\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, texts=None):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        text = self.texts[i]\n",
    "        return self.tokenizer.tokenizer(text)\n",
    "    \n",
    "    def __len__(self, i):\n",
    "        return len(texts)\n",
    "    \n",
    "    def get_tensors(self, texts: Union[str, List[str]], target_styles: Union[int, List[int]], device=None):\n",
    "        if type(texts) == str:\n",
    "            texts = [texts]\n",
    "            target_styles = [target_styles]\n",
    "        else:\n",
    "            assert type(target_styles) == list\n",
    "        assert len(texts) == len(target_styles), f'length of texts ({len(texts)}) must be equal to length of target_styles ({len(target_styles)})'\n",
    "        token_ids = [self.tokenizer.tokenize(text) for text in texts]\n",
    "        token_ids = self.tokenizer.pad(token_ids)\n",
    "        token_ids = torch.tensor(token_ids)\n",
    "        x_mask = (token_ids != 0).long()\n",
    "        x_len = [len(ts) for ts in token_ids]\n",
    "        y_neg = torch.tensor(target_styles).reshape(-1, 1)\n",
    "        y_mask = (y_neg == 0).long()\n",
    "        y_len = [1 for _ in range(len(y_neg))]\n",
    "        token_ids, x_mask, y_neg, y_mask = to_device(token_ids, x_mask, y_neg, y_mask)\n",
    "        return token_ids, x_mask, x_len, y_neg, y_mask, y_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  4, 142, 143, 656, 101,  11, 142]], device='cuda:0'),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       " [7],\n",
       " tensor([[0]], device='cuda:0'),\n",
       " tensor([[1]], device='cuda:0'),\n",
       " [1])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MyDataset(tokenizer)\n",
    "dataset.get_tensors(\"i do n't know what to do\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   4,  142,  143,  656,  101,   11,  142],\n",
       "         [ 845,   50,    0,    0,    0,    0,    0],\n",
       "         [ 170, 4702,   56,    1,    0,    0,    0]], device='cuda:0'),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0]], device='cuda:0'),\n",
       " [7, 7, 7],\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0]], device='cuda:0'),\n",
       " tensor([[1],\n",
       "         [1],\n",
       "         [1]], device='cuda:0'),\n",
       " [1, 1, 1])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MyDataset(tokenizer)\n",
    "dataset.get_tensors([\"i do n't know what to do\", \"Hello there\", \"My pleasure you motherfucker\"], [0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = dataset.get_tensors([\"i do n't know what to do\", \"Hello there\", \"My pleasure you motherfucker\"], [0, 0, 0])\n",
    "hs = model.translate(*tensors, beam_size=hparams.beam_size, max_len=hparams.max_len, poly_norm_m=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 142, 143, 656, 101, 11, 142, 8],\n",
       " [845, 50, 3405, 1864, 4680, 1915, 8],\n",
       " [1184, 174]]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i do n't know what to do .\",\n",
       " 'hello there sanitary frustrating chances tomorrow .',\n",
       " 'unacceptable !']"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda h: ' '.join([data.src_i2w[w] for w in h]), hs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
