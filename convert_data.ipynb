{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Style Transfer\n",
    "Converts data to style transfer format required by deep-latent-sequence-model.\n",
    "\n",
    "We want to perform yelp to sentiment treebank style transfer and back. Since both of these are english language (altough different domains), we will use shared vocabulary. TODO: is there an option to use separate vocabularies? Should be, since authors were doing serbian-bosnian translation (but it probably was shared vocab too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from read_utils import read_sentiment_treebank\n",
    "from style_transfer import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>splitset_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>0.69444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>0.83333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>0.51389</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>0.73611</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>0.86111</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_index                                           sentence  \\\n",
       "0               1  The Rock is destined to be the 21st Century 's...   \n",
       "1               2  The gorgeously elaborate continuation of `` Th...   \n",
       "2               3                     Effective but too-tepid biopic   \n",
       "3               4  If you sometimes like to go to the movies to h...   \n",
       "4               5  Emerges as something rare , an issue movie tha...   \n",
       "\n",
       "   sentiment  splitset_label  \n",
       "0    0.69444               1  \n",
       "1    0.83333               1  \n",
       "2    0.51389               2  \n",
       "3    0.73611               2  \n",
       "4    0.86111               2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_df = read_sentiment_treebank('data/stanfordSentimentTreebank')\n",
    "sst_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444101, 63483, 126670)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_train = get_dataset('yelp', 'train')['text'].values\n",
    "yelp_dev = get_dataset('yelp', 'dev')['text'].values\n",
    "yelp_test = get_dataset('yelp', 'test')['text'].values\n",
    "len(yelp_train), len(yelp_dev), len(yelp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folder structure we want:\n",
    "* sentiment\n",
    "   * train_0.txt\n",
    "   * train_0.attr\n",
    "   * dev_0.txt\n",
    "   * dev_0.attr\n",
    "   * test_0.txt\n",
    "   * test_0.txt\n",
    "   * text.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_path = 'data/sentiment'\n",
    "if not os.path.exists(sentiment_path):\n",
    "    os.mkdir(sentiment_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "sst_train, sst_other = train_test_split(sst_df['sentence'].values, train_size=0.8)\n",
    "sst_dev, sst_test = train_test_split(sst_other, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9484, 1185, 1186)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sst_train), len(sst_dev), len(sst_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9790910193238312"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - len(sst_train) / (len(sst_train) + len(yelp_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.826339097427244"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yelp_train) / len(sst_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = np.concatenate([sst_train, yelp_train])\n",
    "sst_attr = ['sentiment_treebank' for _ in sst_train]\n",
    "yelp_attr = ['yelp' for _ in yelp_train]\n",
    "attr = np.array(sst_attr + yelp_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>attr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20552</th>\n",
       "      <td>i have never received worse customer service .</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321139</th>\n",
       "      <td>i loved those .</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203933</th>\n",
       "      <td>drinks are good and the cocktail list is long .</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  attr\n",
       "20552    i have never received worse customer service .  yelp\n",
       "321139                                  i loved those .  yelp\n",
       "203933  drinks are good and the cocktail list is long .  yelp"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.DataFrame({'text': text, 'attr': attr})\n",
    "text_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "for text in text_df['text'].values:\n",
    "    vocab.update(text.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21814"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 9599, number of words: 3967766\n"
     ]
    }
   ],
   "source": [
    "n_words = 0\n",
    "yelp_vocab = Counter()\n",
    "for text in yelp_train:\n",
    "    words = text.lower().split()\n",
    "    yelp_vocab.update(words)\n",
    "    n_words += len(words)\n",
    "print(f'vocabulary size: {len(yelp_vocab)}, number of words: {n_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 17517, number of words: 181484\n"
     ]
    }
   ],
   "source": [
    "n_words = 0\n",
    "sst_vocab = Counter()\n",
    "for text in sst_train:\n",
    "    words = text.lower().split()\n",
    "    sst_vocab.update(words)\n",
    "    n_words += len(words)\n",
    "print(f'vocabulary size: {len(sst_vocab)}, number of words: {n_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though yelp train dataset is 46 times bigger than stanford sentiment treebank, it has richer vocabulary. It may be simply because of longer sentences, but yelp still has 20 times more words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monti\n",
      "monti 's used to be so good ... now it 's awful .\n",
      "\n",
      "monti\n",
      "we had n't been to monti 's in quite awhile .\n",
      "\n",
      "monti\n",
      "something 's very wrong at monti 's .\n",
      "\n",
      "caffe\n",
      "caffe boa is about as exciting as bulk trash pickup day .\n",
      "\n",
      "caffe\n",
      "caffe boa used to have the best happy hour on mill .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for text in yelp_test:\n",
    "    words = text.lower().split()\n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            print(word)\n",
    "            print(text)\n",
    "            print()\n",
    "            cnt += 1\n",
    "            break\n",
    "    if cnt >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some proper nouns are not included in the vocabulary. That means, that in theory a model would not be able to represent all words. How is it able to copy words though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11330"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_count = 4\n",
    "vocab_cut = {word: count for word, count in vocab.items() if count >= min_count}\n",
    "len(vocab_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>\\n', '<unk>\\n', '<s>\\n', '</s>\\n', '!\\n', '#\\n', '$\\n', '%\\n', '&\\n', \"'\\n\", \"''\\n\", \"'60s\\n\", \"'70s\\n\", \"'burgh\\n\", \"'d\\n\"]\n"
     ]
    }
   ],
   "source": [
    "vocab_words = ['<pad>\\n', '<unk>\\n', '<s>\\n', '</s>\\n'] + sorted([word + '\\n' for word in vocab_cut])\n",
    "print(vocab_words[:15])\n",
    "\n",
    "with open(os.path.join(sentiment_path, 'text.vocab'), 'w') as f:\n",
    "    f.writelines(vocab_words)\n",
    "attr_vocab = ['yelp\\n', 'sentiment_treebank\\n']\n",
    "with open(os.path.join(sentiment_path, 'attr.vocab'), 'w') as f:\n",
    "    f.writelines(attr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    (yelp_train, 'train_0', 'yelp'), \n",
    "    (yelp_dev, 'dev_0', 'yelp'), \n",
    "    (yelp_test, 'test_0', 'yelp'), \n",
    "    (sst_train, 'train_1', 'sentiment_treebank'), \n",
    "    (sst_dev, 'dev_1', 'sentiment_treebank'), \n",
    "    (sst_test, 'test_1', 'sentiment_treebank')\n",
    "]\n",
    "for texts, name, attr in files:\n",
    "    text_lines = [text + '\\n' for text in texts]\n",
    "    with open(os.path.join(sentiment_path, name + '.txt'), 'w') as f:\n",
    "        f.writelines(text_lines)\n",
    "    attr_lines = [attr + '\\n' for _ in texts]\n",
    "    with open(os.path.join(sentiment_path, name + '.attr'), 'w') as f:\n",
    "        f.writelines(attr_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ('train', 'dev', 'test'):\n",
    "    subfiles = list()\n",
    "    for texts, name, attr in files:\n",
    "        if split in name:\n",
    "            subfiles.append((texts, name, attr))\n",
    "    text_all = np.concatenate([texts for texts, _, _ in subfiles])\n",
    "    attr_all = [attr for texts, _, attr in subfiles for _ in texts]\n",
    "    with open(os.path.join(sentiment_path, split + '.txt'), 'w') as f:\n",
    "        text_lines = [text + '\\n' for text in text_all]\n",
    "        f.writelines(text_lines)\n",
    "    with open(os.path.join(sentiment_path, split + '.attr'), 'w') as f:\n",
    "        attr_lines = [attr + '\\n' for attr in attr_all]\n",
    "        assert len(attr_lines) == len(text_lines)\n",
    "        f.writelines(attr_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1185\n"
     ]
    }
   ],
   "source": [
    "SAMPLE = min(len(sst_dev), len(sst_test))\n",
    "print(SAMPLE)\n",
    "np.random.seed(42)\n",
    "for split in ('dev', 'test'):\n",
    "    subfiles = list()\n",
    "    for texts, name, attr in files:\n",
    "        if split in name:\n",
    "            subfiles.append((np.random.choice(texts, SAMPLE, replace=False), name, attr))\n",
    "    text_all = np.concatenate([texts for texts, _, _ in subfiles])\n",
    "    attr_all = [attr for texts, _, attr in subfiles for _ in texts]\n",
    "    with open(os.path.join(sentiment_path, split + '_li.txt'), 'w') as f:\n",
    "        text_lines = [text + '\\n' for text in text_all]\n",
    "        f.writelines(text_lines)\n",
    "    with open(os.path.join(sentiment_path, split + '_li.attr'), 'w') as f:\n",
    "        attr_lines = [attr + '\\n' for attr in attr_all]\n",
    "        assert len(attr_lines) == len(text_lines)\n",
    "        f.writelines(attr_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
